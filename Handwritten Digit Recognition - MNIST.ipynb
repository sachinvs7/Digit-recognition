{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digit Recognition - MNIST\n",
    "\n",
    "## Made to understand in depth on how to practically implement CNNs (Convolutional Neural Networks). Tested with the classic MNIST data set of handwritten digits and explores two models - one with a straightforwrd CNN and another with multiple layers.\n",
    "\n",
    "### Follow each cell to get details on why certain values are set and tweaks are made.\n",
    "\n",
    "The goal is to develop a deep learning model capable of recognizing any digit from 0-9 when presented with a digit image. Each image in the data set is 28 by 28 pixel squared (784 pixels total). There are 60k training images and 10k testing ones. Since it's a digit recognition task, there are a total of 10 classes (0-9) and results are obtained via prediction error rate (Inverted classification accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required libraries and modules, Since I'm using TensorFlow, the image dimension ordering was set to 'tf', If you're using Theano, which is the default dimension ordering for keras, it should be 'th'. Dimension ordering for Tensorflow : (height,width,pixels/channels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeding the random number generator with a constant value for repeatability of results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset and reshaping it to meet needs, also reducing pixel value precision to 32 bit(salvage memory) which is default in Keras. Normalizing or scaling is done to preserve proper functionality in the neural network, since the data set consists of gray scale images (0-255), pixels are normalized between 0-1 by dividing each value with 255. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding the outputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the first deep learning model,32 feature detectors, pool size is 2x2, a regularization layer using dropout called Dropout is configured to randomly exclude 20% of neurons in the layer in order to reduce overfitting, hidden layer has 128 neurons from convention and the final output layer has 10 neurons for each class, the model is trained using logarithmic loss and the ADAM gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building, fitting and evaluating the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 41s - loss: 0.2265 - acc: 0.9349 - val_loss: 0.0745 - val_acc: 0.9771\n",
      "Epoch 2/10\n",
      " - 6s - loss: 0.0709 - acc: 0.9782 - val_loss: 0.0472 - val_acc: 0.9841\n",
      "Epoch 3/10\n",
      " - 6s - loss: 0.0503 - acc: 0.9848 - val_loss: 0.0419 - val_acc: 0.9863\n",
      "Epoch 4/10\n",
      " - 6s - loss: 0.0398 - acc: 0.9874 - val_loss: 0.0392 - val_acc: 0.9876\n",
      "Epoch 5/10\n",
      " - 6s - loss: 0.0317 - acc: 0.9903 - val_loss: 0.0358 - val_acc: 0.9886\n",
      "Epoch 6/10\n",
      " - 6s - loss: 0.0260 - acc: 0.9919 - val_loss: 0.0332 - val_acc: 0.9903\n",
      "Epoch 7/10\n",
      " - 6s - loss: 0.0220 - acc: 0.9928 - val_loss: 0.0346 - val_acc: 0.9898\n",
      "Epoch 8/10\n",
      " - 6s - loss: 0.0190 - acc: 0.9939 - val_loss: 0.0331 - val_acc: 0.9892\n",
      "Epoch 9/10\n",
      " - 6s - loss: 0.0161 - acc: 0.9949 - val_loss: 0.0295 - val_acc: 0.9905\n",
      "Epoch 10/10\n",
      " - 6s - loss: 0.0129 - acc: 0.9960 - val_loss: 0.0296 - val_acc: 0.9907\n",
      "CNN Error: 0.93%\n"
     ]
    }
   ],
   "source": [
    "model = baseline_model()\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell repeats the procedure again but with multiple convolutional and hidden layers included in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.3850 - acc: 0.8822 - val_loss: 0.0816 - val_acc: 0.9756\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.1007 - acc: 0.9688 - val_loss: 0.0592 - val_acc: 0.9795\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.0729 - acc: 0.9778 - val_loss: 0.0417 - val_acc: 0.9864\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.0595 - acc: 0.9814 - val_loss: 0.0381 - val_acc: 0.9882\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.0489 - acc: 0.9845 - val_loss: 0.0356 - val_acc: 0.9891\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 0.0448 - acc: 0.9860 - val_loss: 0.0286 - val_acc: 0.9901\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.0383 - acc: 0.9881 - val_loss: 0.0333 - val_acc: 0.9882\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.0369 - acc: 0.9881 - val_loss: 0.0283 - val_acc: 0.9908\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.0315 - acc: 0.9903 - val_loss: 0.0260 - val_acc: 0.9912\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.0302 - acc: 0.9900 - val_loss: 0.0277 - val_acc: 0.9903\n",
      "Large CNN Error: 0.97%\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "def larger_model():\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dense(50, activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "model = larger_model()\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
